{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel, Multi-Objective BO in BoTorch with qEHVI and qParEGO\n",
    "\n",
    "In this tutorial, we illustrate how to implement a simple multi-objective (MO) Bayesian Optimization (BO) closed loop in BoTorch.\n",
    "\n",
    "We use the parallel ParEGO ($q$ParEGO) [1] and parallel Expected Hypervolume Improvement ($q$EHVI) [1]  acquisition functions to optimize a  synthetic Branin-Currin test function. The two objectives are\n",
    "\n",
    "$$f^{(1)}(x_1\\text{'}, x_2\\text{'}) = (x_2\\text{'} - \\frac{5.1}{4 \\pi^ 2} (x_1\\text{'})^2 + \\frac{5}{\\pi} x_1\\text{'} - r)^2 + 10 (1-\\frac{1}{8 \\pi}) \\cos(x_1\\text{'}) + 10$$\n",
    "\n",
    "$$f^{(2)}(x_1, x_2) = \\bigg[1 - \\exp\\bigg(-\\frac{1} {(2x_2)}\\bigg)\\bigg] \\frac{2300 x_1^3 + 1900x_1^2 + 2092 x_1 + 60}{100 x_1^3 + 500x_1^2 + 4x_1 + 20}$$\n",
    "\n",
    "where $x_1, x_2 \\in [0,1]$, $x_1\\text{'} = 15x_1 - 5$, and $x_2\\text{'} = 15x_2$ (parameter values can be found in `botorch/test_functions/multi_objective.py`).\n",
    "\n",
    "Since botorch assumes a maximization of all objectives, we seek to find the pareto frontier, the set of optimal trade-offs where improving one metric means deteriorating another.\n",
    "\n",
    "[1] [Samuel Daulton, Maximillian Balandat, Eytan Bakshy. Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization. ArXiv e-prints, 2020.](https://arxiv.org/abs/2006.05078)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set dtype and device\n",
    "Note: $q$EHVI aggressively exploits parallel hardware and is much faster when run on a GPU. See [1] for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "tkwargs = {\n",
    "    \"dtype\": torch.double,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.test_functions.multi_objective import BraninCurrin\n",
    "\n",
    "problem = BraninCurrin(negate=True).to(**tkwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform objective to target (-9, -3)\n",
    "def transform_objective(x):\n",
    "    return -torch.abs(x - torch.tensor([[-9, -3]], **tkwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -24.1300,   -7.4051],\n",
       "        [-145.8722,   -4.0053]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem(torch.tensor([[0.5, 0.5], [1, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -15.1300,   -4.4051],\n",
       "        [-136.8722,   -1.0053]], dtype=torch.float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_objective(problem(torch.tensor([[0.5, 0.5], [1, 1]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-18.,  -6.], dtype=torch.float64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem.ref_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model initialization\n",
    "\n",
    "We use a multi-output `SingleTaskGP` to model the two objectives with a homoskedastic Gaussian likelihood with an inferred noise level.\n",
    "\n",
    "The models are initialized with $2(d+1)=6$ points drawn randomly from $[0,1]^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.models.gp_regression import SingleTaskGP\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "from gpytorch.mlls.exact_marginal_log_likelihood import ExactMarginalLogLikelihood\n",
    "from botorch.utils.transforms import unnormalize\n",
    "from botorch.utils.sampling import draw_sobol_samples\n",
    "\n",
    "def generate_initial_data(n=6):\n",
    "    # generate training data\n",
    "    train_x = draw_sobol_samples(bounds=problem.bounds,n=1, q=n, seed=torch.randint(1000000, (1,)).item()).squeeze(0)\n",
    "    train_obj = transform_objective(problem(train_x))\n",
    "    return train_x, train_obj\n",
    "    \n",
    "def initialize_model(train_x, train_obj):\n",
    "    # define models for objective and constraint\n",
    "    model = SingleTaskGP(train_x, train_obj, outcome_transform=Standardize(m=train_obj.shape[-1]))\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    return mll, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a helper function that performs the essential BO step for $q$EHVI\n",
    "The helper function below initializes the $q$EHVI acquisition function, optimizes it, and returns the batch $\\{x_1, x_2, \\ldots x_q\\}$ along with the observed function values. \n",
    "\n",
    "For this example, we'll use a small batch of $q=4$. Passing the keyword argument `sequential=True` to the function `optimize_acqf`specifies that candidates should be optimized in a sequential greedy fashion (see [1] for details why this is important). A simple initialization heuristic is used to select the 20 restart initial locations from a set of 1024 random points. Multi-start optimization of the acquisition function is performed using LBFGS-B with exact gradients computed via auto-differentiation.\n",
    "\n",
    "**Reference Point**\n",
    "\n",
    "$q$EHVI requires specifying a reference point, which is the lower bound on the objectives used for computing hypervolume. In this tutorial, we assume the reference point is known. In practice the reference point can be set 1) using domain knowledge to be slightly worse than the lower bound of objective values, where the lower bound is the minimum acceptable value of interest for each objective, or 2) using a dynamic reference point selection strategy.\n",
    "\n",
    "**Partitioning the Non-dominated Space into disjoint rectangles**\n",
    "\n",
    "$q$EHVI requires partitioning the non-dominated space into disjoint rectangles (see [1] for details). \n",
    "\n",
    "*Note:* `NondominatedPartitioning` *will be very slow when 1) there are a lot of points on the pareto frontier and 2) there are >3 objectives.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.optim.optimize import optimize_acqf, optimize_acqf_list\n",
    "from botorch.acquisition.objective import GenericMCObjective\n",
    "from botorch.utils.multi_objective.scalarization import get_chebyshev_scalarization\n",
    "from botorch.utils.multi_objective.box_decomposition import NondominatedPartitioning\n",
    "from botorch.acquisition.multi_objective.monte_carlo import qExpectedHypervolumeImprovement\n",
    "from botorch.utils.sampling import sample_simplex\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "standard_bounds = torch.zeros(2, problem.dim, **tkwargs)\n",
    "standard_bounds[1] = 1\n",
    "\n",
    "\n",
    "def optimize_qehvi_and_get_observation(model, train_obj, sampler):\n",
    "    \"\"\"Optimizes the qEHVI acquisition function, and returns a new candidate and observation.\"\"\"\n",
    "    # partition non-dominated space into disjoint rectangles\n",
    "    partitioning = NondominatedPartitioning(num_outcomes=problem.num_objectives, Y=train_obj)\n",
    "    acq_func = qExpectedHypervolumeImprovement(\n",
    "        model=model,\n",
    "        ref_point=[-3, -1],  # use known reference point \n",
    "        partitioning=partitioning,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "    # optimize\n",
    "    candidates, _ = optimize_acqf(\n",
    "        acq_function=acq_func,\n",
    "        bounds=standard_bounds,\n",
    "        q=BATCH_SIZE,\n",
    "        num_restarts=20,\n",
    "        raw_samples=1024,  # used for intialization heuristic\n",
    "        options={\"batch_limit\": 5, \"maxiter\": 200, \"nonnegative\": True},\n",
    "        sequential=True,\n",
    "    )\n",
    "    # observe new values \n",
    "    new_x =  unnormalize(candidates.detach(), bounds=problem.bounds)\n",
    "    new_obj = transform_objective(problem(new_x))\n",
    "    return new_x, new_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a helper function that performs the essential BO step for $q$ParEGO\n",
    "The helper function below similarly initializes $q$ParEGO, optimizes it, and returns the batch $\\{x_1, x_2, \\ldots x_q\\}$ along with the observed function values. \n",
    "\n",
    "$q$ParEGO uses random augmented chebyshev scalarization with the `qExpectedImprovement` acquisition function. In the parallel setting ($q>1$), each candidate is optimized in sequential greedy fashion using a different random scalarization (see [1] for details).\n",
    "\n",
    "To do this, we create a list of `qExpectedImprovement` acquisition functions, each with different random scalarization weights. The `optimize_acqf_list` method sequentially generates one candidate per acquisition function and conditions the next candidate (and acquisition function) on the previously selected pending candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_qparego_and_get_observation(model, train_obj, sampler):\n",
    "    \"\"\"Samples a set of random weights for each candidate in the batch, performs sequential greedy optimization \n",
    "    of the qParEGO acquisition function, and returns a new candidate and observation.\"\"\"\n",
    "    acq_func_list = []\n",
    "    for _ in range(BATCH_SIZE):\n",
    "        weights = sample_simplex(problem.num_objectives, **tkwargs).squeeze()\n",
    "        objective = GenericMCObjective(get_chebyshev_scalarization(weights=weights, Y=train_obj))\n",
    "        acq_func = qExpectedImprovement(  # pyre-ignore: [28]\n",
    "            model=model,\n",
    "            objective=objective,\n",
    "            best_f=objective(train_obj).max().item(),\n",
    "            sampler=sampler,\n",
    "        )\n",
    "        acq_func_list.append(acq_func)\n",
    "    # optimize\n",
    "    candidates, _ = optimize_acqf_list(\n",
    "        acq_function_list=acq_func_list,\n",
    "        bounds=standard_bounds,\n",
    "        num_restarts=20,\n",
    "        raw_samples=1024,  # used for intialization heuristic\n",
    "        options={\"batch_limit\": 5, \"maxiter\": 200},\n",
    "    )\n",
    "    # observe new values \n",
    "    new_x =  unnormalize(candidates.detach(), bounds=problem.bounds)\n",
    "    new_obj = transform_objective(problem(new_x))\n",
    "    return new_x, new_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Bayesian Optimization loop with $q$EHVI and $q$ParEGO\n",
    "The Bayesian optimization \"loop\" for a batch size of $q$ simply iterates the following steps:\n",
    "1. given a surrogate model, choose a batch of points $\\{x_1, x_2, \\ldots x_q\\}$\n",
    "2. observe $f(x)$ for each $x$ in the batch \n",
    "3. update the surrogate model. \n",
    "\n",
    "\n",
    "Just for illustration purposes, we run three trials each of which do `N_BATCH=25` rounds of optimization. The acquisition function is approximated using `MC_SAMPLES=128` samples.\n",
    "\n",
    "*Note*: Running this may take a little while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial  1 of 3 Iter 10/100: 2.6160859589005327\n",
      "Iter 20/100: 2.4531662523272413\n",
      "Iter 30/100: 2.3238755833539506\n",
      "Iter 40/100: 2.1017456886861554\n",
      "Iter 10/100: 2.615533025184702\n",
      "Iter 20/100: 2.4524721697450453\n",
      "Iter 30/100: 2.3282608888569674\n",
      "Iter 40/100: 2.139777272989797\n",
      "Iter 10/100: 2.6160859589005327\n",
      "Iter 20/100: 2.4531662523272413\n",
      "Iter 30/100: 2.3238755833539506\n",
      "Iter 40/100: 2.1017456886861554\n",
      "Iter 10/100: 2.615533025184702\n",
      "Iter 20/100: 2.4524721697450453\n",
      "Iter 30/100: 2.3282608888569674\n",
      "Iter 40/100: 2.139777272989797\n"
     ]
    }
   ],
   "source": [
    "from botorch import fit_gpytorch_model\n",
    "from botorch.optim.fit import fit_gpytorch_torch\n",
    "from botorch.acquisition.monte_carlo import qExpectedImprovement, qNoisyExpectedImprovement\n",
    "from botorch.sampling.samplers import SobolQMCNormalSampler\n",
    "from botorch.exceptions import BadInitialCandidatesWarning\n",
    "from botorch.utils.multi_objective.pareto import is_non_dominated\n",
    "from botorch.utils.multi_objective.hypervolume import Hypervolume\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=BadInitialCandidatesWarning)\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "N_TRIALS = 3\n",
    "N_BATCH = 25\n",
    "MC_SAMPLES = 128\n",
    "\n",
    "verbose = False\n",
    "\n",
    "hvs_qparego_all, hvs_qehvi_all, hvs_random_all = [], [], []\n",
    "\n",
    "hv = Hypervolume(ref_point=torch.tensor([-10.0, -1.0]))\n",
    "\n",
    "# average over multiple trials\n",
    "for trial in range(1, N_TRIALS + 1):\n",
    "    torch.manual_seed(trial)\n",
    "    \n",
    "    print(f\"\\nTrial {trial:>2} of {N_TRIALS} \", end=\"\")\n",
    "    hvs_qparego, hvs_qehvi, hvs_random = [], [], []\n",
    "    \n",
    "    # call helper functions to generate initial training data and initialize model\n",
    "    train_x_qparego, train_obj_qparego = generate_initial_data(n=6)\n",
    "    mll_qparego, model_qparego = initialize_model(train_x_qparego, train_obj_qparego)\n",
    "    \n",
    "    train_x_qehvi, train_obj_qehvi = train_x_qparego, train_obj_qparego\n",
    "    train_x_random, train_obj_random = train_x_qparego, train_obj_qparego\n",
    "    # compute hypervolume \n",
    "    mll_qehvi, model_qehvi = initialize_model(train_x_qehvi, train_obj_qehvi)\n",
    "    \n",
    "    # compute pareto front\n",
    "    pareto_mask = is_non_dominated(train_obj_qparego)\n",
    "    pareto_y = train_obj_qparego[pareto_mask]\n",
    "    # compute hypervolume\n",
    "    \n",
    "    volume = hv.compute(pareto_y)\n",
    "    \n",
    "    hvs_qparego.append(volume)\n",
    "    hvs_qehvi.append(volume)\n",
    "    hvs_random.append(volume)\n",
    "    \n",
    "    # run N_BATCH rounds of BayesOpt after the initial random batch\n",
    "    for iteration in range(1, N_BATCH + 1):    \n",
    "        \n",
    "        t0 = time.time()\n",
    "        \n",
    "        # fit the models\n",
    "        fit_gpytorch_model(mll_qparego, optimizer=fit_gpytorch_torch)\n",
    "        fit_gpytorch_model(mll_qehvi, optimizer=fit_gpytorch_torch)\n",
    "        \n",
    "        # define the qEI and qNEI acquisition modules using a QMC sampler\n",
    "        qparego_sampler = SobolQMCNormalSampler(num_samples=MC_SAMPLES)\n",
    "        qehvi_sampler = SobolQMCNormalSampler(num_samples=MC_SAMPLES)\n",
    "        \n",
    "        # optimize acquisition functions and get new observations\n",
    "        new_x_qparego, new_obj_qparego = optimize_qparego_and_get_observation(\n",
    "            model_qparego, train_obj_qparego, qparego_sampler\n",
    "        )\n",
    "        new_x_qehvi, new_obj_qehvi = optimize_qehvi_and_get_observation(\n",
    "            model_qehvi, train_obj_qehvi, qehvi_sampler\n",
    "        )\n",
    "        new_x_random, new_obj_random = generate_initial_data(n=BATCH_SIZE)\n",
    "                \n",
    "        # update training points\n",
    "        train_x_qparego = torch.cat([train_x_qparego, new_x_qparego])\n",
    "        train_obj_qparego = torch.cat([train_obj_qparego, new_obj_qparego])\n",
    "\n",
    "        train_x_qehvi = torch.cat([train_x_qehvi, new_x_qehvi])\n",
    "        train_obj_qehvi = torch.cat([train_obj_qehvi, new_obj_qehvi])\n",
    "    \n",
    "        train_x_random = torch.cat([train_x_random, new_x_random])\n",
    "        train_obj_random = torch.cat([train_obj_random, new_obj_random])\n",
    "        \n",
    "\n",
    "        # update progress\n",
    "        for hvs_list, train_obj in zip(\n",
    "            (hvs_random, hvs_qparego, hvs_qehvi), \n",
    "            (train_obj_random, train_obj_qparego, train_obj_qehvi),\n",
    "        ):\n",
    "            # compute pareto front\n",
    "            pareto_mask = is_non_dominated(train_obj)\n",
    "            pareto_y = train_obj[pareto_mask]\n",
    "            # compute hypervolume\n",
    "            volume = hv.compute(pareto_y)\n",
    "            hvs_list.append(volume)\n",
    "\n",
    "        # reinitialize the models so they are ready for fitting on next iteration\n",
    "        # Note: we find improved performance from not warm starting the model hyperparameters\n",
    "        # using the hyperparameters from the previous iteration\n",
    "        mll_qparego, model_qparego = initialize_model(train_x_qparego, train_obj_qparego)\n",
    "        mll_qehvi, model_qehvi = initialize_model(train_x_qehvi, train_obj_qehvi)\n",
    "        \n",
    "        t1 = time.time()\n",
    "        \n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"\\nBatch {iteration:>2}: Hypervolume (random, qParEGO, qEHVI) = \"\n",
    "                f\"({hvs_random[-1]:>4.2f}, {hvs_qparego[-1]:>4.2f}, {hvs_qehvi[-1]:>4.2f}), \"\n",
    "                f\"time = {t1-t0:>4.2f}.\", end=\"\"\n",
    "            )\n",
    "        else:\n",
    "            print(\".\", end=\"\")\n",
    "   \n",
    "    hvs_qparego_all.append(hvs_qparego)\n",
    "    hvs_qehvi_all.append(hvs_qehvi)\n",
    "    hvs_random_all.append(hvs_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the results\n",
    "The plot below shows the a common metric of multi-objective optimization performance, the log hypervolume difference: the log difference between the hypervolume of the true pareto front and the hypervolume of the approximate pareto front identified by each algorithm. The log hypervolume difference is plotted at each step of the optimization for each of the algorithms. The confidence intervals represent the variance at that step in the optimization across the trial runs. The variance across optimization runs is quite high, so in order to get a better estimate of the average performance one would have to run a much larger number of trials `N_TRIALS` (we avoid this here to limit the runtime of this tutorial). \n",
    "\n",
    "The plot show that $q$EHVI vastly outperforms the $q$ParEGO and Sobol baselines and has very low variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def ci(y):\n",
    "    return 1.96 * y.std(axis=0) / np.sqrt(N_TRIALS)\n",
    "\n",
    "\n",
    "iters = np.arange(N_BATCH + 1) * BATCH_SIZE\n",
    "log_hv_difference_qparego = np.log10(10 - np.asarray(hvs_qparego_all))\n",
    "log_hv_difference_qehvi = np.log10(10 - np.asarray(hvs_qehvi_all))\n",
    "log_hv_difference_rnd = np.log10(10 - np.asarray(hvs_random_all))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "ax.errorbar(iters, log_hv_difference_rnd.mean(axis=0), yerr=ci(log_hv_difference_rnd), label=\"Sobol\", linewidth=1.5)\n",
    "ax.errorbar(iters, log_hv_difference_qparego.mean(axis=0), yerr=ci(log_hv_difference_qparego), label=\"qParEGO\", linewidth=1.5)\n",
    "ax.errorbar(iters, log_hv_difference_qehvi.mean(axis=0), yerr=ci(log_hv_difference_qehvi), label=\"qEHVI\", linewidth=1.5)\n",
    "ax.set(xlabel='number of observations (beyond initial points)', ylabel='Log Hypervolume Difference')\n",
    "ax.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### plot the observations colored by iteration\n",
    "\n",
    "To examine optimization process from another perspective, we plot the collected observations under each algorithm where the color corresponds to the BO iteration at which the point was collected. The plot on the right for $q$EHVI shows that the $q$EHVI quickly identifies the pareto front and most of its evaluations are very close to the pareto front. $q$ParEGO also identifies has many observations close to the pareto front, but relies on optimizing random scalarizations, which is a less principled way of optimizing the pareto front compared to $q$EHVI, which explicitly attempts focuses on improving the pareto front. Sobol generates random points and has few points close to the pareto front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.cm import ScalarMappable\n",
    "fig, axes = plt.subplots(1, 3, figsize=(17, 5))\n",
    "algos = [\"Sobol\", \"qParEGO\", \"qEHVI\"]\n",
    "cm = plt.cm.get_cmap('viridis')\n",
    "\n",
    "batch_number = torch.cat([torch.zeros(6), torch.arange(1, N_BATCH+1).repeat(BATCH_SIZE, 1).t().reshape(-1)]).numpy()\n",
    "for i, train_obj in enumerate((train_obj_random, train_obj_qparego, train_obj_qehvi)):\n",
    "    sc = axes[i].scatter(train_obj[:, 0].cpu().numpy(), train_obj[:,1].cpu().numpy(), c=batch_number, alpha=0.8)\n",
    "    axes[i].set_title(algos[i])\n",
    "    axes[i].set_xlabel(\"Objective 1\")\n",
    "    axes[i].set_xlim(-260, 5)\n",
    "    axes[i].set_ylim(-15, 0)\n",
    "axes[0].set_ylabel(\"Objective 2\")\n",
    "norm = plt.Normalize(batch_number.min(), batch_number.max())\n",
    "sm =  ScalarMappable(norm=norm, cmap=cm)\n",
    "sm.set_array([])\n",
    "fig.subplots_adjust(right=0.9)\n",
    "cbar_ax = fig.add_axes([0.93, 0.15, 0.01, 0.7])\n",
    "cbar = fig.colorbar(sm, cax=cbar_ax)\n",
    "cbar.ax.set_title(\"Iteration\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
